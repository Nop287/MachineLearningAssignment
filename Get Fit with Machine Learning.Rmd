---
title: "Get Fit with Machine Learning"
author: "Norbert Widmann"
date: "July 23, 2015"
output: html_document
---

<!--  
x echo # MachineLearningAssignment >> README.md
x git init
x git add README.md
git commit -m "first commit"
x git remote add origin https://github.com/Nop287/MachineLearningAssignment.git
git push -u origin master

Has the student submitted a github repo?
Does the submission build a machine learning algorithm to predict activity quality from activity monitors?
Do the authors describe what they expect the out of sample error to be and estimate the error appropriately with cross-validation?

Save the data for another day:
saveRDS(modFit, file="modFit.rds")
saveRDS(test_small, file="test_small.rds")
saveRDS(train_small, file="train_small.rds")
-->


Introduction
------------

This work is based on a dataset on weight lifting exercises used in the paper "Qualitative Activity Recognition of Weight Lifting Exercises" by E. Velloso et. al. Focus of the work here is classifying the quality of exercise execution into five different classes (A to E) based on data from wearable sensors.

Data Set
--------

Looking at the dataset and reading the related paper we see that some preprocessing has already been done. Basically the time series data has been averaged using a sliding window and only aggregated values for the sensors are given.

The basic structure is that the value classe is to be predicted and there are 159 potential features. Importing the csv data using read.csv results in features getting the wrong classes. So first we convert user_name and classe as factors. Then we standardize the relevant features to numeric.

The data contains a serious amount of NA values. These we set to zero for determing columns which are near zero values. We exclude these columnus from further analysis as they contain no relevant information.

```{r warning=FALSE}
library(AppliedPredictiveModeling)
library(caret)
training <- read.csv("pml-training.csv", stringsAsFactors=FALSE)
training$user_name = factor(training$user_name)
training$classe = factor(training$classe)
training[,8:159] <- sapply(training[,8:159], as.numeric)
tmp <- training
tmp[is.na(tmp)] <- 0
nzv <- nearZeroVar(tmp)
rm(tmp)
training <- training[,-nzv]
```

Now we check for features which are highly correlated and remove them. These measures are taken to reduce the number of features to be used in the model and therefore reduce the required training time for the model. We also remove the first few attributes (id, timestamps) since they are not relevant as features to predict the classification. We retain the user_name which should be relevant for prediction.

```{r}
classe <- training[, 59]
user <- training[,2]
training <- training[,7:dim(training)[2] - 1]
highlyCorFeat <- findCorrelation(cor(training), cutoff = .75)
training <- training[,-highlyCorFeat]
```

Next we add additional dummy features for the name of the person performing the exercise since this probably is a highly relevant feature. Finally we add classe as the value to be predicter to the data set. After this we are ready to train a model and we reduced the features to consider from 159 to 39 which should result in a significant reduction in training time for the model.

```{r}
training$adelmo <- 0
training$adelmo[user == "adelmo"] <- 1
training$carlitos <- 0
training$carlitos[user == "carlitos"] <- 1
training$charles <- 0
training$charles[user == "charles"] <- 1
training$eurico <- 0
training$eurico[user == "eurico"] <- 1
training$jeremy <- 0
training$jeremy[user == "jeremy"] <- 1
training$pedro <- 0
training$pedro[user == "pedro"] <- 1
training$classe <- classe
```

Model Training and Evaluation
-----------------------------

Now we are ready to train the model. Since we are predicting a categorical variable and we assume a non-linear relationship between features and result we use a random forest as the model.

To assess the quality of the model we split up the training data in two separate data sets: One for training the model and one for testing the model and assessing the quality.

```{r}
set.seed(4711)
train_test <- createDataPartition(y=training$classe, p=.6, list=FALSE)
train_small <- training[train_test,]
test_small <- training[-train_test,]
```

We train our random forest on the train_small test set. We use cross validation to optimise the parameters.

```{r}
ctrl = trainControl(method="cv",number=5)
# Note: The model has been trained and stored beforhand.
# modFit <- train(classe ~ ., data=train_small, method="rf",
#                 trControl=ctrl, prox=TRUE, allowParallel=TRUE)
# saveRDS(modFit, file="modFit.rds")
modFit <- readRDS("modFit.rds")
```

Given that we used cross validation we can look at the information of the model to get some indication of the out of sample error. The accuracy is very high at about 99,7% for the best mtry parameter.

```{r}
modFit
```

Now we assess the model quality using the test_small data set to predict the out of sample error. This confirms the accuracy given by the cross validation at 99,7%. As this is a very high accuracy we do not evaluate any further models.

```{r}
result <- predict(modFit, test_small)
confusionMatrix(test_small$classe, result)
```

Finally we predict the actual test set to get the final results of our model building. For that we have to prepare the final test set according to the training data set, i.e. convert everything to numbers and add a categorial variabel for the test subject.

```{r}
final_test <- read.csv("pml-testing.csv", stringsAsFactors=FALSE)
final_test[,8:159] <- sapply(final_test[,8:159], as.numeric)
user <- final_test[,2]
final_test <- final_test[,7:dim(final_test)[2]]
final_test$adelmo <- 0
final_test$adelmo[user == "adelmo"] <- 1
final_test$carlitos <- 0
final_test$carlitos[user == "carlitos"] <- 1
final_test$charles <- 0
final_test$charles[user == "charles"] <- 1
final_test$eurico <- 0
final_test$eurico[user == "eurico"] <- 1
final_test$jeremy <- 0
final_test$jeremy[user == "jeremy"] <- 1
final_test$pedro <- 0
final_test$pedro[user == "pedro"] <- 1
final_result <- predict(modFit, final_test)
answers <- as.character(final_result)
answers
```

We upload the predictions. Our model has predicted 20 out of 20 classes correctly.

```{r echo=FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
```
